{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda9c034",
   "metadata": {},
   "source": [
    "# Build an ML Pipeline for Airfoil noise prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15cb2df",
   "metadata": {},
   "source": [
    "**Airfoil**: A cross-sectional shape of a wing, blade, or sail that is designed to generate lift when air flows over it. In aeronautics, airfoils are critical components in aircraft wings, helicopter rotors, propellers, and turbine blades. The shape of an airfoil directly affects its aerodynamic performance, including lift generation, drag characteristics, and importantly, the noise it produces as air flows over its surface. Understanding and predicting airfoil noise is essential for designing quieter, more efficient aircraft and reducing environmental noise pollution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635f4a2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a29a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-4.0.1-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pyspark]m1/2\u001b[0m [pyspark]\n",
      "\u001b[1A\u001b[2KSuccessfully installed py4j-0.10.9.9 pyspark-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting findspark\n",
      "  Using cached findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Using cached findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n",
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0280edb9",
   "metadata": {},
   "source": [
    "## Part 1 - Perform ETL activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ee674",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c550f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cced3326",
   "metadata": {},
   "source": [
    "### Create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a360792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/14 11:16:47 WARN Utils: Your hostname, maishuji, resolves to a loopback address: 127.0.1.1; using 192.168.0.18 instead (on interface wlp4s0)\n",
      "25/12/14 11:16:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/14 11:16:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Airfoil Noise Prediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ef81f",
   "metadata": {},
   "source": [
    "### Load the csv file into a datadrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325ce99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-14 11:19:12--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/NASA_airfoil_noise_raw.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60682 (59K) [text/csv]\n",
      "Saving to: ‘./../data/raw/NASA_airfoil_noise_raw.csv’\n",
      "\n",
      "./../data/raw/NASA_ 100%[===================>]  59.26K   384KB/s    in 0.2s    \n",
      "\n",
      "2025-12-14 11:19:13 (384 KB/s) - ‘./../data/raw/NASA_airfoil_noise_raw.csv’ saved [60682/60682]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/NASA_airfoil_noise_raw.csv -O ./../data/raw/NASA_airfoil_noise_raw.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b16f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"./../data/raw/NASA_airfoil_noise_raw.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4b6b4",
   "metadata": {},
   "source": [
    "### Print top 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2394d3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "|Frequency|AngleOfAttack|ChordLength|FreeStreamVelocity|SuctionSideDisplacement|SoundLevel|\n",
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "|      800|          0.0|     0.3048|              71.3|             0.00266337|   126.201|\n",
      "|     1000|          0.0|     0.3048|              71.3|             0.00266337|   125.201|\n",
      "|     1250|          0.0|     0.3048|              71.3|             0.00266337|   125.951|\n",
      "|     1600|          0.0|     0.3048|              71.3|             0.00266337|   127.591|\n",
      "|     2000|          0.0|     0.3048|              71.3|             0.00266337|   127.461|\n",
      "+---------+-------------+-----------+------------------+-----------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761aec07",
   "metadata": {},
   "source": [
    "### Print the total number of rows in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3308a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before removing duplicates and nulls: 1522\n"
     ]
    }
   ],
   "source": [
    "rowcount1 = df.count()\n",
    "print(f\"Row count before removing duplicates and nulls: {rowcount1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8b338",
   "metadata": {},
   "source": [
    "### Drop all the duplicate rows from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb5d4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b72f5",
   "metadata": {},
   "source": [
    "### Print the total number of rows in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eda21a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count after removing duplicates: 1503\n"
     ]
    }
   ],
   "source": [
    "rowcount2 = df.count()\n",
    "print(f\"Row count after removing duplicates: {rowcount2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454848d",
   "metadata": {},
   "source": [
    "### Drop all the rows that contain null values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "351a32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705714cb",
   "metadata": {},
   "source": [
    "### Print the total number of rows in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66e0a9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count after removing nulls: 1499\n"
     ]
    }
   ],
   "source": [
    "rowcount3 = df.count()\n",
    "print(f\"Row count after removing nulls: {rowcount3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e4b03",
   "metadata": {},
   "source": [
    "### Rename the column \"SoundLevel\" to \"SoundLevelDecibels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d653b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"SoundLevel\", \"SoundLevelDecibels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed1ead",
   "metadata": {},
   "source": [
    "### Save the dataframe in parquet format, name the file as \"NASA_airfoil_noise_cleaned.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54214677",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"./../data/processed/NASA_airfoil_noise_cleaned.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0af06a",
   "metadata": {},
   "source": [
    "### Part 1 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "219dc316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 - Evaluation\n",
      "Total rows =  1522\n",
      "Total rows after dropping duplicate rows =  1503\n",
      "Total rows after dropping duplicate rows and rows with null values =  1499\n",
      "New column name =  SoundLevelDecibels\n",
      "NASA_airfoil_noise_cleaned.parquet exists : True\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 1 - Evaluation\")\n",
    "\n",
    "print(\"Total rows = \", rowcount1)\n",
    "print(\"Total rows after dropping duplicate rows = \", rowcount2)\n",
    "print(\"Total rows after dropping duplicate rows and rows with null values = \", rowcount3)\n",
    "print(\"New column name = \", df.columns[-1])\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"NASA_airfoil_noise_cleaned.parquet exists :\", os.path.isdir(\"./../data/processed/NASA_airfoil_noise_cleaned.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e8f208",
   "metadata": {},
   "source": [
    "## Part 2 - Create a Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d0ce1",
   "metadata": {},
   "source": [
    "### Load data from the .parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642bb775",
   "metadata": {},
   "source": [
    "### Print the total number of rows in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16380688",
   "metadata": {},
   "source": [
    "### Define the VectorAssembler pipeline stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b1800",
   "metadata": {},
   "source": [
    "### Define the StandardScaler pipeline stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea82dfda",
   "metadata": {},
   "source": [
    "### Define the Model creation pipeline stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e36b2",
   "metadata": {},
   "source": [
    "### Build the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff89a6f",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3311209",
   "metadata": {},
   "source": [
    "### Fit the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1a959",
   "metadata": {},
   "source": [
    "### Part 2 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2072e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Part 2 - Evaluation\")\n",
    "print(\"Total rows = \", rowcount4)\n",
    "ps = [str(x).split(\"_\")[0] for x in pipeline.getStages()]\n",
    "\n",
    "print(\"Pipeline Stage 1 = \", ps[0])\n",
    "print(\"Pipeline Stage 2 = \", ps[1])\n",
    "print(\"Pipeline Stage 3 = \", ps[2])\n",
    "\n",
    "print(\"Label column = \", lr.getLabelCol())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
